{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Training, Evaluation & Artifact Generation\n",
    "\n",
    "**Objective:** Load cleaned data (where `regionidzip` and `fips` are prepared as categorical model inputs), define & train models, evaluate, analyze importance (aggregating OHE features to original names like `regionidzip`, `fips`), save final artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup, Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np; import matplotlib.pyplot as plt; import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder; from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline; from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LinearRegression, Lasso;\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "import lightgbm as lgb; from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib; import os; import time; import warnings; import gc;\n",
    "try: import shap\n",
    "except ImportError: print(\"SHAP library not installed. SHAP analysis will be skipped.\"); shap = None\n",
    "\n",
    "warnings.filterwarnings('ignore'); pd.set_option('display.max_columns',100); pd.set_option('display.max_rows',100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid'); %matplotlib inline\n",
    "\n",
    "MODEL_DIR = 'saved_models'\n",
    "STATIC_FOLDER = 'static'\n",
    "PLOTS_SUBFOLDER = 'plots'\n",
    "PLOTS_DIR = os.path.join(STATIC_FOLDER, PLOTS_SUBFOLDER)\n",
    "TOP_N_FEATURES = 25\n",
    "CLEANED_DATA_CSV_PATH = './data/df_cleaned_for_modeling.csv'\n",
    "CLEANED_DATA_ARTIFACTS_PATH = os.path.join(MODEL_DIR, 'data_processing_artifacts.joblib')\n",
    "\n",
    "os.makedirs(MODEL_DIR,exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR,exist_ok=True)\n",
    "print(\"Part 2 Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Cleaned Data and Artifacts from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading Cleaned Data and Artifacts from Part 1 ---\")\n",
    "try:\n",
    "    df_cleaned = pd.read_csv(CLEANED_DATA_CSV_PATH)\n",
    "    print(f\"Loaded df_cleaned from CSV, shape: {df_cleaned.shape}\")\n",
    "    part1_artifacts = joblib.load(CLEANED_DATA_ARTIFACTS_PATH)\n",
    "    numeric_features = part1_artifacts.get('numeric_features_for_model', [])\n",
    "    categorical_features = part1_artifacts.get('categorical_features_for_model', []) # Should now include 'regionidzip', 'fips'\n",
    "    expected_columns_before_preprocessing = part1_artifacts.get('expected_columns_for_model', []) \n",
    "    available_zips_clean = part1_artifacts.get('available_zips_clean', []) \n",
    "    available_fips_clean = part1_artifacts.get('available_fips_clean', []) # Added in Notebook 1\n",
    "    default_suggestions = part1_artifacts.get('default_suggestions', {}) \n",
    "    field_descriptions = part1_artifacts.get('field_descriptions', {})\n",
    "    id_value_mappings = part1_artifacts.get('id_value_mappings', {})\n",
    "    print(f\"Loaded Part 1 artifacts: {len(numeric_features)} numeric, {len(categorical_features)} categorical model features identified.\")\n",
    "    if 'regionidzip' in categorical_features: print(\"'regionidzip' IS in categorical_features and will be OHE'd.\")\n",
    "    else: print(\"CRITICAL WARNING: 'regionidzip' NOT found in categorical_features from Part 1 artifacts! Check Part 1 logic.\")\n",
    "    if 'fips' in categorical_features: print(\"'fips' IS in categorical_features and will be OHE'd.\")\n",
    "    else: print(\"CRITICAL WARNING: 'fips' NOT found in categorical_features from Part 1 artifacts! Check Part 1 logic.\")\n",
    "except Exception as e: print(f\"ERROR loading Part 1 data/artifacts: {e}\"); raise e\n",
    "\n",
    "# Ensure features intended as categorical for the model are string type before preprocessor\n",
    "for col in categorical_features:\n",
    "    if col in df_cleaned.columns and df_cleaned[col].dtype != 'object' and df_cleaned[col].dtype != pd.StringDtype():\n",
    "        df_cleaned[col] = df_cleaned[col].astype(str)\n",
    "\n",
    "display(df_cleaned.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 EDA on Loaded Cleaned Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- EDA on Loaded Data (Example: SqFt vs Price) ---\")\n",
    "if 'calculatedfinishedsquarefeet' in df_cleaned.columns and 'saleprice' in df_cleaned.columns:\n",
    "    sample_eda = df_cleaned.sample(min(2000, len(df_cleaned)), random_state=1)\n",
    "    plt.figure(figsize=(8,5)); sns.scatterplot(data=sample_eda, x='calculatedfinishedsquarefeet', y='saleprice', alpha=0.5)\n",
    "    plt.title('SqFt vs SalePrice (Sample from df_cleaned)'); plt.xlabel('Calculated Finished Square Feet'); plt.ylabel('Sale Price');\n",
    "    eda_plot_path = os.path.join(PLOTS_DIR, 'sqft_vs_price_sample_eda_part2.png')\n",
    "    try: plt.savefig(eda_plot_path); print(f\"- EDA plot saved: {eda_plot_path}\")\n",
    "    except Exception as e: print(f\"Error saving EDA plot: {e}\")\n",
    "    plt.show(); plt.close()\n",
    "else: print(\"Skipping SqFt vs Price EDA (required columns not found).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Defining Preprocessing Pipeline ---\")\n",
    "numeric_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='median')),('scaler',StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),('onehot',OneHotEncoder(handle_unknown='ignore',sparse_output=False))])\n",
    "transformers_list = []\n",
    "if numeric_features: transformers_list.append(('num', numeric_transformer, numeric_features))\n",
    "if categorical_features: transformers_list.append(('cat', categorical_transformer, categorical_features)) # Will process 'regionidzip', 'fips'\n",
    "\n",
    "preprocessor = None\n",
    "if transformers_list:\n",
    "    preprocessor=ColumnTransformer(transformers=transformers_list,remainder='drop',verbose_feature_names_out=False)\n",
    "    print(\"Preprocessor defined. It will use specified numeric/categorical features (incl. 'regionidzip', 'fips') and drop others.\")\n",
    "    from sklearn import set_config; set_config(display='diagram'); display(preprocessor)\n",
    "else: \n",
    "    print(\"ERROR: No numeric or categorical features specified for the preprocessor! Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Final Data Preparation for Modeling (Splitting & Fitting Preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Final Data Prep for Modeling ---\")\n",
    "assert 'df_cleaned' in locals() and not df_cleaned.empty, \"df_cleaned missing or empty!\"\n",
    "assert preprocessor is not None, \"Preprocessor not defined!\"\n",
    "assert 'saleprice' in df_cleaned.columns, \"Target 'saleprice' missing in df_cleaned!\"\n",
    "\n",
    "X = df_cleaned[expected_columns_before_preprocessing].copy() \n",
    "y = df_cleaned['saleprice'].copy()\n",
    "print(f\"Shape of X for splitting (from df_cleaned using expected_columns): {X.shape}\")\n",
    "if 'regionidzip' in X.columns and 'regionidzip' in categorical_features: print(\"'regionidzip' IS in X and categorical_features, will be OHE'd by preprocessor.\")\n",
    "if 'fips' in X.columns and 'fips' in categorical_features: print(\"'fips' IS in X and categorical_features, will be OHE'd by preprocessor.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Fitting preprocessor & transforming data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"Processed train data shape: {X_train_processed.shape}, Processed test data shape: {X_test_processed.shape}\")\n",
    "\n",
    "final_model_input_feature_names = None;\n",
    "try:\n",
    "    num_tf_obj = preprocessor.named_transformers_.get('num')\n",
    "    actual_numeric_feats_to_num_tf = list(preprocessor.transformers_[0][2]) if num_tf_obj and preprocessor.transformers_ and preprocessor.transformers_[0][0]=='num' else []\n",
    "    cat_tf_obj = preprocessor.named_transformers_.get('cat')\n",
    "    cat_transformer_idx = next((i for i,t in enumerate(preprocessor.transformers_) if t[0]=='cat'), -1)\n",
    "    actual_categorical_feats_to_cat_tf = []\n",
    "    ohe_feature_names = []\n",
    "    if cat_tf_obj and cat_transformer_idx != -1:\n",
    "        actual_categorical_feats_to_cat_tf = list(preprocessor.transformers_[cat_transformer_idx][2])\n",
    "        ohe_step_in_cat_pipeline = cat_tf_obj.named_steps.get('onehot')\n",
    "        if ohe_step_in_cat_pipeline and hasattr(ohe_step_in_cat_pipeline,'get_feature_names_out') and actual_categorical_feats_to_cat_tf:\n",
    "            ohe_feature_names = list(ohe_step_in_cat_pipeline.get_feature_names_out(actual_categorical_feats_to_cat_tf))\n",
    "    final_model_input_feature_names = list(actual_numeric_feats_to_num_tf) + ohe_feature_names\n",
    "    if final_model_input_feature_names and len(final_model_input_feature_names) == X_train_processed.shape[1]:\n",
    "        print(f\"Successfully extracted {len(final_model_input_feature_names)} final model input feature names after preprocessing.\")\n",
    "    else: raise ValueError(f\"Feature name count mismatch. Expected {X_train_processed.shape[1]}, got {len(final_model_input_feature_names if final_model_input_feature_names else [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Detailed feature name extraction failed: {e}. Using generic names.\")\n",
    "    final_model_input_feature_names = [f'model_feature_{i}' for i in range(X_train_processed.shape[1])]\n",
    "\n",
    "joblib.dump(preprocessor, os.path.join(MODEL_DIR, 'preprocessor_fitted_part2.joblib')); print(\"- Fitted Preprocessor (from Part 2) saved.\")\n",
    "joblib.dump(final_model_input_feature_names or [], os.path.join(MODEL_DIR, 'feature_names_final_model_inputs.joblib')); print(\"- Final model input feature names (post-preprocessing) saved.\")\n",
    "\n",
    "del X, y, X_train, X_test, df_cleaned; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Model Building (LightGBM default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Defining Models ---\")\n",
    "models = {\n",
    "    \"Ridge\": Ridge(alpha=5.0, random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=150, max_depth=20, min_samples_leaf=10, n_jobs=-1, random_state=42, max_features=0.6),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1, importance_type='gain', verbosity=-1)\n",
    "}\n",
    "ENABLE_STACKING = True\n",
    "if ENABLE_STACKING:\n",
    "    estimators_stack = [\n",
    "        ('ridge', Ridge(alpha=5.0, random_state=42)), \n",
    "        ('rf', RandomForestRegressor(n_estimators=150, max_depth=20, min_samples_leaf=10, n_jobs=-1, random_state=42, max_features=0.6)), \n",
    "        ('lgbm', lgb.LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1, importance_type='gain', verbosity=-1))\n",
    "    ]\n",
    "    stacking_model = StackingRegressor(estimators=estimators_stack, final_estimator=RidgeCV(alphas=np.logspace(-2,2,10)), cv=3, n_jobs=-1)\n",
    "    models[\"Stacked Ensemble (LGBM)\"] = stacking_model\n",
    "print(f\"Models to train: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Training and Evaluation ---\")\n",
    "results = {}; trained_models = {}\n",
    "assert 'X_train_processed' in locals() and 'X_test_processed' in locals(), \"Processed data missing!\"\n",
    "assert 'y_train' in locals() and 'y_test' in locals(), \"Target variable missing!\"\n",
    "for name, model_obj in models.items():\n",
    "    start_time=time.time();print(f\"\\nTraining {name}...\");model_obj.fit(X_train_processed,y_train);trained_models[name]=model_obj\n",
    "    y_pred_test=model_obj.predict(X_test_processed);y_pred_train=model_obj.predict(X_train_processed)\n",
    "    rmse_t=np.sqrt(mean_squared_error(y_test,y_pred_test));mae_t=mean_absolute_error(y_test,y_pred_test);r2_t=r2_score(y_test,y_pred_test);rmse_tr=np.sqrt(mean_squared_error(y_train,y_pred_train))\n",
    "    results[name]={'RMSE_Test':rmse_t,'MAE_Test':mae_t,'R2_Test':r2_t,'RMSE_Train':rmse_tr,'Time (s)':time.time()-start_time}\n",
    "    print(f\"-> {name}: Test RMSE:{rmse_t:,.0f}, R2:{r2_t:.4f} | Train RMSE:{rmse_tr:,.0f} | Time: {results[name]['Time (s)']:.2f}s\")\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='RMSE_Test');print(\"\\n--- Model Performance Summary ---\");display(results_df)\n",
    "if not results_df.empty:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid'); fig,axes=plt.subplots(2,1,figsize=(12,10),sharex=False); idx=np.arange(len(results_df)); bw=0.35;\n",
    "    axes[0].bar(idx-bw/2,results_df['RMSE_Test'],bw,label='Test RMSE',color='c'); axes[0].bar(idx+bw/2,results_df['RMSE_Train'],bw,label='Train RMSE',color='lightsteelblue',alpha=0.7);\n",
    "    axes[0].set_ylabel('RMSE'); axes[0].set_title('Model RMSE (Sorted by Test RMSE) & R2 Performance'); axes[0].legend(); axes[0].set_xticks(idx); axes[0].set_xticklabels(results_df.index,rotation=30,ha='right');\n",
    "    for i, rect in enumerate(axes[0].patches): height = rect.get_height(); axes[0].text(rect.get_x() + rect.get_width()/2., height + 0.01*height, f'{height:,.0f}', ha='center', va='bottom', fontsize=7)\n",
    "    r2_df_sorted=results_df.sort_values('R2_Test',ascending=False); r2_indices = np.arange(len(r2_df_sorted));\n",
    "    axes[1].bar(r2_indices,r2_df_sorted['R2_Test'],color='mediumseagreen'); axes[1].set_ylabel('R2_Test Score'); axes[1].axhline(0,color='k',lw=0.8,ls='--');\n",
    "    axes[1].set_xticks(r2_indices); axes[1].set_xticklabels(r2_df_sorted.index,rotation=30,ha='right');\n",
    "    for i, rect in enumerate(axes[1].patches): height = rect.get_height(); axes[1].text(rect.get_x() + rect.get_width()/2., height + (0.01 if height >=0 else -0.05) , f'{height:.3f}', ha='center', va='bottom' if height >=0 else 'top', fontsize=7)\n",
    "    plt.tight_layout(); perf_plot_path = os.path.join(PLOTS_DIR,'model_performance_comparison_part2.png');\n",
    "    try: plt.savefig(perf_plot_path); print(f'- Performance plot saved: {perf_plot_path}.')\n",
    "    except Exception as e: print(f\"Error saving performance plot: {e}\")\n",
    "    plt.show();plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Feature Importance & SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Feature Importance & SHAP Analysis ---\")\n",
    "best_model_name=None; final_model_object=None; importances_processed=None; \n",
    "feature_importance_df_processed=None; aggregated_importance_df=None; feature_importance_data_for_ui_table=None\n",
    "field_order_preference_from_importance = list(expected_columns_before_preprocessing if 'expected_columns_before_preprocessing' in locals() else [])\n",
    "top_n_display = TOP_N_FEATURES\n",
    "\n",
    "if 'results_df' in locals() and not results_df.empty: best_model_name=results_df['RMSE_Test'].dropna().idxmin(); final_model_object=trained_models.get(best_model_name); print(f\"Best model selected: {best_model_name}\")\n",
    "\n",
    "if final_model_object:\n",
    "    try: \n",
    "        if hasattr(final_model_object,'feature_importances_'): importances_processed = final_model_object.feature_importances_\n",
    "        elif isinstance(final_model_object,StackingRegressor): \n",
    "            if hasattr(final_model_object.final_estimator_, 'coef_'): importances_processed = np.abs(final_model_object.final_estimator_.coef_.squeeze())\n",
    "            else: lgbm_base=final_model_object.named_estimators_.get('lgbm'); rf_base=final_model_object.named_estimators_.get('rf')\n",
    "            if lgbm_base and hasattr(lgbm_base,'feature_importances_'):importances_processed=lgbm_base.feature_importances_;print(\"- Stack: LGBM base importance.\")\n",
    "            elif rf_base and hasattr(rf_base,'feature_importances_'):importances_processed=rf_base.feature_importances_;print(\"- Stack: RF base importance.\")\n",
    "        elif hasattr(final_model_object,'coef_'): importances_processed = np.abs(final_model_object.coef_.squeeze())\n",
    "        if importances_processed is not None: print(f\"Extracted {len(importances_processed)} importance values from the model.\")\n",
    "    except Exception as e:print(f\"Error extracting importances: {e}\")\n",
    "\n",
    "if 'final_model_input_feature_names' not in locals() or not final_model_input_feature_names: print(\"ERROR: 'final_model_input_feature_names' missing!\"); importances_processed=None\n",
    "\n",
    "if importances_processed is not None and final_model_input_feature_names and len(importances_processed)==len(final_model_input_feature_names):\n",
    "    feature_importance_df_processed=pd.DataFrame({'feature':final_model_input_feature_names,'importance':importances_processed}).sort_values('importance',ascending=False)\n",
    "    print(\"Created DataFrame of processed feature importances (model's direct inputs like 'fips_value').\")\n",
    "    if not feature_importance_df_processed.empty:\n",
    "        aggregated_importances_map={}\n",
    "        original_numeric_features_for_agg = locals().get('numeric_features', []) \n",
    "        original_categorical_features_for_agg = locals().get('categorical_features', []) \n",
    "        for orig_feat_name in original_numeric_features_for_agg:\n",
    "            if orig_feat_name in feature_importance_df_processed['feature'].values: aggregated_importances_map[orig_feat_name] = feature_importance_df_processed.loc[feature_importance_df_processed['feature'] == orig_feat_name, 'importance'].sum()\n",
    "        for orig_cat_feat_name in original_categorical_features_for_agg: # This loop includes 'regionidzip', 'fips'\n",
    "            ohe_cols_for_this_cat_feat = [proc_feat_name for proc_feat_name in final_model_input_feature_names if proc_feat_name.startswith(orig_cat_feat_name + \"_\")]\n",
    "            if not ohe_cols_for_this_cat_feat and orig_cat_feat_name in final_model_input_feature_names: ohe_cols_for_this_cat_feat = [orig_cat_feat_name]\n",
    "            current_sum = feature_importance_df_processed[feature_importance_df_processed['feature'].isin(ohe_cols_for_this_cat_feat)]['importance'].sum()\n",
    "            if current_sum > 1e-9: aggregated_importances_map[orig_cat_feat_name] = aggregated_importances_map.get(orig_cat_feat_name, 0) + current_sum\n",
    "        if aggregated_importances_map:\n",
    "            aggregated_importance_df=pd.DataFrame(list(aggregated_importances_map.items()),columns=['original_feature','aggregated_importance']).sort_values('aggregated_importance',ascending=False)\n",
    "            print(\"Created DataFrame of aggregated feature importances (summing OHE columns to original names like 'fips').\")\n",
    "            if not aggregated_importance_df.empty:\n",
    "                field_order_preference_from_importance=aggregated_importance_df['original_feature'].tolist()\n",
    "                print(f\"- Field order preference generated from aggregated importances ({len(field_order_preference_from_importance)} features).\")\n",
    "                feature_importance_data_for_ui_table = aggregated_importance_df.head(top_n_display).rename(columns={'original_feature': 'feature', 'aggregated_importance': 'importance'}).to_dict(orient='records')\n",
    "                print(f\"- UI feature importance table data (from aggregated importances) generated.\")\n",
    "else: print(f\"Warning: Importance array length ({len(importances_processed if importances_processed is not None else [])}) and feature names length ({len(final_model_input_feature_names if final_model_input_feature_names else [])}) mismatch or one is missing.\")\n",
    "\n",
    "if not field_order_preference_from_importance: field_order_preference_from_importance = list(expected_columns_before_preprocessing if 'expected_columns_before_preprocessing' in locals() else [])\n",
    "if feature_importance_data_for_ui_table is None: feature_importance_data_for_ui_table=[]\n",
    "\n",
    "if aggregated_importance_df is not None and not aggregated_importance_df.empty:\n",
    "    try: top_n=min(top_n_display,len(aggregated_importance_df)); plt.figure(figsize=(10,max(6,top_n*0.35))); sns.barplot(x='aggregated_importance',y='original_feature',data=aggregated_importance_df.head(top_n),palette='crest_r');plt.title(f'Top {top_n} Aggregated Feature Importances (Original Features)');plt.tight_layout();path=os.path.join(PLOTS_DIR,'feature_importance_aggregated_original_part2.png');plt.savefig(path);print(f\"- Aggregated importance plot (showing 'fips', 'regionidzip') saved: {path}\");plt.show()\n",
    "    except Exception as e:print(f\"ERROR plotting aggregated importances: {e}\")\n",
    "    finally:plt.close()\n",
    "else: print(\"Skipping aggregated importance plot (data unavailable).\")\n",
    "\n",
    "if feature_importance_df_processed is not None and not feature_importance_df_processed.empty:\n",
    "    try: top_n=min(top_n_display,len(feature_importance_df_processed));plt.figure(figsize=(10,max(6,top_n*0.35)));sns.barplot(x='importance',y='feature',data=feature_importance_df_processed.head(top_n),palette='mako');plt.title(f'Top {top_n} Processed Feature Importances ({best_model_name or 'Model'})');plt.tight_layout();path=os.path.join(PLOTS_DIR,'feature_importance_processed_part2.png');plt.savefig(path);print(f\"- Processed importance plot (showing OHE columns like 'fips_value') saved: {path}\");plt.show()\n",
    "    except Exception as e:print(f\"ERROR plotting processed importances: {e}\")\n",
    "    finally:plt.close()\n",
    "else: print(\"Skipping processed importance plot (data unavailable).\")\n",
    "\n",
    "print(\"\\n--- SHAP Analysis Attempt ---\")\n",
    "if shap and best_model_name and final_model_object and 'X_train_processed' in locals() and X_train_processed.shape[0]>0 and 'final_model_input_feature_names' in locals() and final_model_input_feature_names:\n",
    "    try:\n",
    "        X_train_processed_df_for_shap = pd.DataFrame(X_train_processed, columns=final_model_input_feature_names);\n",
    "        sample_size_shap = min(500, X_train_processed_df_for_shap.shape[0])\n",
    "        if sample_size_shap > 0:\n",
    "            X_shap_sample_df = shap.sample(X_train_processed_df_for_shap, sample_size_shap, random_state=42)\n",
    "            print(f\"SHAP for {best_model_name} (sample size: {sample_size_shap})...\");explainer_obj=None;shap_values_output=None;model_for_shap_explanation=final_model_object;shap_model_display_name=best_model_name\n",
    "            if isinstance(final_model_object,StackingRegressor):\n",
    "                lgbm_s=final_model_object.named_estimators_.get('lgbm')\n",
    "                rf_s=final_model_object.named_estimators_.get('rf')\n",
    "                if lgbm_s and hasattr(lgbm_s,'predict'):model_for_shap_explanation=lgbm_s;shap_model_display_name=f\"{best_model_name}(LGBM Base)\";print(\"- Stack:SHAP(LGBM Base)\")\n",
    "                elif rf_s and hasattr(rf_s,'predict'):model_for_shap_explanation=rf_s;shap_model_display_name=f\"{best_model_name}(RF Base)\";print(\"- Stack:SHAP(RF Base)\")\n",
    "            \n",
    "            if isinstance(model_for_shap_explanation,(RandomForestRegressor,GradientBoostingRegressor,lgb.LGBMRegressor)):\n",
    "                explainer_obj=shap.TreeExplainer(model_for_shap_explanation)\n",
    "            elif isinstance(model_for_shap_explanation,(LinearRegression,Ridge,Lasso,RidgeCV)):\n",
    "                explainer_obj=shap.LinearExplainer(model_for_shap_explanation,X_shap_sample_df)\n",
    "            \n",
    "            if explainer_obj:shap_values_output=explainer_obj.shap_values(X_shap_sample_df)\n",
    "            \n",
    "            if shap_values_output is not None:\n",
    "                try:\n",
    "                    shap.summary_plot(shap_values_output,X_shap_sample_df,plot_type=\"bar\",max_display=top_n_display,show=False)\n",
    "                    plt.title(f\"SHAP Bar({shap_model_display_name})\");plt.tight_layout();path=os.path.join(PLOTS_DIR,'shap_summary_bar_part2.png')\n",
    "                    plt.savefig(path);print(f\"- SHAP bar: {path}\");plt.show();plt.close()\n",
    "                except Exception as e:print(f\"ERR SHAP bar:{e}\");plt.close()\n",
    "                try:\n",
    "                    shap.summary_plot(shap_values_output,X_shap_sample_df,max_display=top_n_display,show=False)\n",
    "                    plt.title(f\"SHAP Dots({shap_model_display_name})\");plt.tight_layout();path=os.path.join(PLOTS_DIR,'shap_summary_dots_part2.png')\n",
    "                    plt.savefig(path);print(f\"- SHAP dot: {path}\");plt.show();plt.close()\n",
    "                except Exception as e:print(f\"ERR SHAP dot:{e}\");plt.close()\n",
    "    except Exception as e:print(f\"Error SHAP:{e}\")\n",
    "else:print(\"Skipping SHAP.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Save Final Artifacts for Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 2.9: Saving Final Artifacts for Flask App ---\")\n",
    "joblib.dump(locals().get('id_value_mappings', {}), os.path.join(MODEL_DIR, 'id_value_mappings_final.joblib')); print(\"- ID-to-Value mappings saved.\")\n",
    "joblib.dump(locals().get('field_descriptions', {}), os.path.join(MODEL_DIR, 'field_descriptions_final.joblib')); print(\"- Field descriptions saved.\")\n",
    "joblib.dump(feature_importance_data_for_ui_table or [], os.path.join(MODEL_DIR, 'feature_importance_table_data_final.joblib')); print(f\"- Feature importance table data (aggregated for UI) saved ({len(feature_importance_data_for_ui_table or [])} items).\")\n",
    "joblib.dump(field_order_preference_from_importance or [], os.path.join(MODEL_DIR, 'field_order_preference_final.joblib')); print(f\"- Field order preference list (aggregated for UI) saved ({len(field_order_preference_from_importance or [])} items).\")\n",
    "joblib.dump(locals().get('default_suggestions', {}), os.path.join(MODEL_DIR, 'default_suggestions_final.joblib')); print(\"- Default value suggestions for UI saved.\")\n",
    "joblib.dump(locals().get('available_zips_clean', []), os.path.join(MODEL_DIR, 'available_zips_clean_final.joblib')); print(\"- Available ZIPs for UI saved.\")\n",
    "joblib.dump(locals().get('available_fips_clean', []), os.path.join(MODEL_DIR, 'available_fips_clean_final.joblib')); print(\"- Available FIPS codes for UI saved.\")\n",
    "joblib.dump(locals().get('numeric_features', []), os.path.join(MODEL_DIR, 'numeric_features_for_preprocessor_final.joblib')); print(\"- Original numeric features list (for preprocessor) saved.\")\n",
    "joblib.dump(locals().get('categorical_features', []), os.path.join(MODEL_DIR, 'categorical_features_for_preprocessor_final.joblib')); print(\"- Original categorical features list (for preprocessor, incl. 'regionidzip', 'fips') saved.\")\n",
    "joblib.dump(locals().get('expected_columns_before_preprocessing', []), os.path.join(MODEL_DIR, 'expected_columns_before_preprocessing_final.joblib')); print(\"- List of all raw input columns (before preprocessor) saved.\")\n",
    "\n",
    "if os.path.exists(os.path.join(MODEL_DIR, 'preprocessor_fitted_part2.joblib')): print(\"- Fitted preprocessor already saved.\")\n",
    "if os.path.exists(os.path.join(MODEL_DIR, 'feature_names_final_model_inputs.joblib')): print(\"- Final model input feature names (post-OHE) already saved.\")\n",
    "\n",
    "if best_model_name and final_model_object:\n",
    "    final_model_filename = f'{best_model_name.replace(\" \", \"_\").lower()}_final_model_v_part2.joblib'\n",
    "    final_model_path = os.path.join(MODEL_DIR, final_model_filename)\n",
    "    try: joblib.dump(final_model_object, final_model_path); print(f\"- Best Model '{best_model_name}' saved to {final_model_path}\")\n",
    "    except Exception as e: print(f\"ERROR saving final model: {e}\")\n",
    "else: print(\"Warning: Skipping best model saving.\")\n",
    "\n",
    "print(\"\\n--- All Final Artifact Saving Complete (Part 2) ---\")\n",
    "print(\"--- Notebook Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}